{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJhO1ao63otD",
        "outputId": "5677f0d4-06f2-4872-c9ba-bea4d537a6f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# Change this path\n",
        "image_path = '/content/drive/MyDrive/peddet_2_1.jpg'\n",
        "\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "transform = T.Compose([\n",
        "    T.Resize(800),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406],\n",
        "                [0.229, 0.224, 0.225])\n",
        "])\n",
        "img_tensor = transform(img).unsqueeze(0)  # shape: [1, 3, H, W]\n"
      ],
      "metadata": {
        "id": "CFHMWJbz3qa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import types\n",
        "\n",
        "def make_patched_forward(layer_type):\n",
        "    def patched_forward(self, query, key, value, **kwargs):\n",
        "        embed_dim = self.embed_dim\n",
        "        num_heads = self.num_heads\n",
        "        head_dim = embed_dim // num_heads\n",
        "        scaling = float(head_dim) ** -0.5\n",
        "\n",
        "        # Linear projections\n",
        "        q_proj_weight = self.in_proj_weight[:embed_dim]\n",
        "        k_proj_weight = self.in_proj_weight[embed_dim:2*embed_dim]\n",
        "        v_proj_weight = self.in_proj_weight[2*embed_dim:]\n",
        "\n",
        "        q_proj_bias = self.in_proj_bias[:embed_dim]\n",
        "        k_proj_bias = self.in_proj_bias[embed_dim:2*embed_dim]\n",
        "        v_proj_bias = self.in_proj_bias[2*embed_dim:]\n",
        "\n",
        "        q = F.linear(query, q_proj_weight, q_proj_bias) * scaling\n",
        "        k = F.linear(key,   k_proj_weight, k_proj_bias)\n",
        "        v = F.linear(value, v_proj_weight, v_proj_bias)\n",
        "\n",
        "        # Reshape for heads\n",
        "        seq_len, batch_size, _ = q.shape\n",
        "        q_heads = q.view(seq_len, batch_size, num_heads, head_dim)\n",
        "        k_heads = k.view(k.shape[0], batch_size, num_heads, head_dim)\n",
        "        v_heads = v.view(v.shape[0], batch_size, num_heads, head_dim)\n",
        "\n",
        "        print(f\"\\n {layer_type.upper()} ATTENTION\")\n",
        "        print(f\"Q shape: {q.shape}, K shape: {k.shape}, V shape: {v.shape}\")\n",
        "        print(f\"Per-head shape: Q {q_heads.shape}, K {k_heads.shape}, V {v_heads.shape}\")\n",
        "\n",
        "        # Print value slices\n",
        "        print(\" Q[0, 0, 0, :10]:\", q_heads[0, 0, 0, :10])\n",
        "        print(\" K[0, 0, 0, :10]:\", k_heads[0, 0, 0, :10])\n",
        "        print(\" V[0, 0, 0, :10]:\", v_heads[0, 0, 0, :10])\n",
        "\n",
        "        return torch.nn.functional.multi_head_attention_forward(\n",
        "            query, key, value,\n",
        "            embed_dim, num_heads,\n",
        "            self.in_proj_weight, self.in_proj_bias,\n",
        "            self.bias_k, self.bias_v,\n",
        "            self.add_zero_attn, self.dropout,\n",
        "            self.out_proj.weight, self.out_proj.bias,\n",
        "            training=self.training,\n",
        "            need_weights=kwargs.get(\"need_weights\", False),\n",
        "            attn_mask=kwargs.get(\"attn_mask\", None),\n",
        "            key_padding_mask=kwargs.get(\"key_padding_mask\", None),\n",
        "            use_separate_proj_weight=False\n",
        "        )\n",
        "    return patched_forward\n",
        "\n",
        "def patch_all_attention_layers(model):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.MultiheadAttention):\n",
        "            if \"encoder\" in name:\n",
        "                layer_type = \"encoder\"\n",
        "            elif \"self_attn\" in name:\n",
        "                layer_type = \"decoder_self\"\n",
        "            else:\n",
        "                layer_type = \"decoder_cross\"\n",
        "            print(f\"Patching: {name} --> {layer_type}\")\n",
        "            module.forward = types.MethodType(make_patched_forward(layer_type), module)\n"
      ],
      "metadata": {
        "id": "cyxadlIU3zC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== STEP 4: Load DETR and Run Inference ==========\n",
        "# Load pretrained DETR\n",
        "model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Patch attention layers\n",
        "patch_all_attention_layers(model)\n",
        "\n",
        "# Run inference (prints Q/K/V shapes and per-head shapes)\n",
        "with torch.no_grad():\n",
        "    outputs = model(img_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBlqVxXF5Nnt",
        "outputId": "84a78b51-043c-45ff-fa7b-661a730baa35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_detr_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patching: transformer.encoder.layers.0.self_attn --> encoder\n",
            "Patching: transformer.encoder.layers.1.self_attn --> encoder\n",
            "Patching: transformer.encoder.layers.2.self_attn --> encoder\n",
            "Patching: transformer.encoder.layers.3.self_attn --> encoder\n",
            "Patching: transformer.encoder.layers.4.self_attn --> encoder\n",
            "Patching: transformer.encoder.layers.5.self_attn --> encoder\n",
            "Patching: transformer.decoder.layers.0.self_attn --> decoder_self\n",
            "Patching: transformer.decoder.layers.0.multihead_attn --> decoder_cross\n",
            "Patching: transformer.decoder.layers.1.self_attn --> decoder_self\n",
            "Patching: transformer.decoder.layers.1.multihead_attn --> decoder_cross\n",
            "Patching: transformer.decoder.layers.2.self_attn --> decoder_self\n",
            "Patching: transformer.decoder.layers.2.multihead_attn --> decoder_cross\n",
            "Patching: transformer.decoder.layers.3.self_attn --> decoder_self\n",
            "Patching: transformer.decoder.layers.3.multihead_attn --> decoder_cross\n",
            "Patching: transformer.decoder.layers.4.self_attn --> decoder_self\n",
            "Patching: transformer.decoder.layers.4.multihead_attn --> decoder_cross\n",
            "Patching: transformer.decoder.layers.5.self_attn --> decoder_self\n",
            "Patching: transformer.decoder.layers.5.multihead_attn --> decoder_cross\n",
            "\n",
            " ENCODER ATTENTION\n",
            "Q shape: torch.Size([1125, 1, 256]), K shape: torch.Size([1125, 1, 256]), V shape: torch.Size([1125, 1, 256])\n",
            "Per-head shape: Q torch.Size([1125, 1, 8, 32]), K torch.Size([1125, 1, 8, 32]), V torch.Size([1125, 1, 8, 32])\n",
            " Q[0, 0, 0, :10]: tensor([ 0.0412, -0.5514, -0.5157, -0.0988, -0.2459,  0.4406,  0.2575,  0.1072,\n",
            "         0.5031,  0.1700])\n",
            " K[0, 0, 0, :10]: tensor([-0.3402,  1.5077,  0.5618,  1.2335,  1.5190,  0.2382, -0.2176, -0.4764,\n",
            "        -2.8407,  0.7925])\n",
            " V[0, 0, 0, :10]: tensor([-0.9839,  2.4765,  3.7567,  1.3829,  0.3927,  0.9218,  2.1182,  3.9996,\n",
            "        -1.8837,  2.4350])\n",
            "\n",
            " ENCODER ATTENTION\n",
            "Q shape: torch.Size([1125, 1, 256]), K shape: torch.Size([1125, 1, 256]), V shape: torch.Size([1125, 1, 256])\n",
            "Per-head shape: Q torch.Size([1125, 1, 8, 32]), K torch.Size([1125, 1, 8, 32]), V torch.Size([1125, 1, 8, 32])\n",
            " Q[0, 0, 0, :10]: tensor([ 0.4624,  0.1801, -0.1396,  0.3429,  0.3663,  0.0027, -0.0569, -0.0240,\n",
            "        -0.0804, -0.3453])\n",
            " K[0, 0, 0, :10]: tensor([ 2.8373,  1.1240,  1.0732,  1.5815,  3.6028,  0.0398,  0.3900, -0.4880,\n",
            "         0.6477, -0.6266])\n",
            " V[0, 0, 0, :10]: tensor([-0.6849,  0.7362, -1.1622, -0.9596, -0.0667, -0.1644, -0.0302,  0.1526,\n",
            "         0.6553, -0.7302])\n",
            "\n",
            " ENCODER ATTENTION\n",
            "Q shape: torch.Size([1125, 1, 256]), K shape: torch.Size([1125, 1, 256]), V shape: torch.Size([1125, 1, 256])\n",
            "Per-head shape: Q torch.Size([1125, 1, 8, 32]), K torch.Size([1125, 1, 8, 32]), V torch.Size([1125, 1, 8, 32])\n",
            " Q[0, 0, 0, :10]: tensor([ 0.1281,  0.5254, -0.2545, -0.0374,  0.1731,  0.2189,  0.2975, -0.3116,\n",
            "         0.0592, -0.4504])\n",
            " K[0, 0, 0, :10]: tensor([-0.4546,  4.4346,  1.3383,  2.7382, -1.5938,  3.7036,  0.8345,  0.5574,\n",
            "         0.6970, -2.9696])\n",
            " V[0, 0, 0, :10]: tensor([-0.3620, -0.2633,  0.0381,  0.3345, -0.4141,  0.0432, -0.1677,  0.3147,\n",
            "         0.1418,  0.1744])\n",
            "\n",
            " ENCODER ATTENTION\n",
            "Q shape: torch.Size([1125, 1, 256]), K shape: torch.Size([1125, 1, 256]), V shape: torch.Size([1125, 1, 256])\n",
            "Per-head shape: Q torch.Size([1125, 1, 8, 32]), K torch.Size([1125, 1, 8, 32]), V torch.Size([1125, 1, 8, 32])\n",
            " Q[0, 0, 0, :10]: tensor([-0.4773, -0.0348, -0.3413, -0.2015, -0.0530,  0.0373,  0.2927, -0.2564,\n",
            "        -0.4358, -0.3759])\n",
            " K[0, 0, 0, :10]: tensor([-0.4180, -1.9509, -4.2933, -1.9324, -1.9747,  1.2986, -2.1786, -0.4254,\n",
            "        -1.0864, -2.3187])\n",
            " V[0, 0, 0, :10]: tensor([ 0.8724, -0.1265, -0.2617,  0.6257,  0.2408, -0.1139, -0.0311, -0.3178,\n",
            "         0.0856, -0.1167])\n",
            "\n",
            " ENCODER ATTENTION\n",
            "Q shape: torch.Size([1125, 1, 256]), K shape: torch.Size([1125, 1, 256]), V shape: torch.Size([1125, 1, 256])\n",
            "Per-head shape: Q torch.Size([1125, 1, 8, 32]), K torch.Size([1125, 1, 8, 32]), V torch.Size([1125, 1, 8, 32])\n",
            " Q[0, 0, 0, :10]: tensor([-0.0732, -0.3075, -0.0047,  0.0281,  0.1679,  0.0965,  0.2032, -0.1223,\n",
            "        -0.2825, -0.3701])\n",
            " K[0, 0, 0, :10]: tensor([ 1.1240,  0.9009,  1.6810, -0.0027, -0.0409, -0.7670, -1.0761, -0.5722,\n",
            "         1.0701,  0.6183])\n",
            " V[0, 0, 0, :10]: tensor([ 0.4923, -0.4071,  0.4055, -1.5021,  0.6760, -0.1917, -0.3370,  0.7051,\n",
            "        -0.4217, -0.4917])\n",
            "\n",
            " ENCODER ATTENTION\n",
            "Q shape: torch.Size([1125, 1, 256]), K shape: torch.Size([1125, 1, 256]), V shape: torch.Size([1125, 1, 256])\n",
            "Per-head shape: Q torch.Size([1125, 1, 8, 32]), K torch.Size([1125, 1, 8, 32]), V torch.Size([1125, 1, 8, 32])\n",
            " Q[0, 0, 0, :10]: tensor([ 0.2361,  0.0718,  0.1199, -0.1024, -0.1643,  0.2864, -0.2690,  0.2079,\n",
            "         0.0217, -0.0123])\n",
            " K[0, 0, 0, :10]: tensor([ 1.4944, -2.7219,  0.9141, -2.3281, -2.5515, -0.2513,  0.2895,  0.9149,\n",
            "        -3.4105,  3.7745])\n",
            " V[0, 0, 0, :10]: tensor([-0.1596,  0.0434, -0.5014,  0.0735, -0.0289, -1.5757, -0.5738, -0.2999,\n",
            "         0.7026, -0.0139])\n",
            "\n",
            " DECODER_SELF ATTENTION\n",
            "Q shape: torch.Size([100, 1, 256]), K shape: torch.Size([100, 1, 256]), V shape: torch.Size([100, 1, 256])\n",
            "Per-head shape: Q torch.Size([100, 1, 8, 32]), K torch.Size([100, 1, 8, 32]), V torch.Size([100, 1, 8, 32])\n",
            " Q[0, 0, 0, :10]: tensor([-0.4418,  0.4502, -0.3816,  0.0275, -0.2405,  0.2532,  0.1943,  0.1778,\n",
            "         0.6278,  0.2321])\n",
            " K[0, 0, 0, :10]: tensor([ 0.4154, -0.1413, -1.8048, -0.7175, -0.1650, -0.2214,  0.0508,  1.3940,\n",
            "         0.8101, -0.2700])\n",
            " V[0, 0, 0, :10]: tensor([-0.0032,  0.0928, -0.0084, -0.0409,  0.0225, -0.0471,  0.0021, -0.1039,\n",
            "         0.0108,  0.0002])\n",
            "\n",
            " DECODER_CROSS ATTENTION\n",
            "Q shape: torch.Size([100, 1, 256]), K shape: torch.Size([1125, 1, 256]), V shape: torch.Size([1125, 1, 256])\n",
            "Per-head shape: Q torch.Size([100, 1, 8, 32]), K torch.Size([1125, 1, 8, 32]), V torch.Size([1125, 1, 8, 32])\n",
            " Q[0, 0, 0, :10]: tensor([-1.0848,  0.5247, -0.0607,  0.7111,  0.3408, -0.0804, -0.0525,  0.0903,\n",
            "         0.3814, -1.2829])\n",
            " K[0, 0, 0, :10]: tensor([ 1.7730,  1.6071,  0.0982,  0.3004, -1.2827,  0.0637, -0.6003,  0.9002,\n",
            "        -0.5779,  0.5038])\n",
            " V[0, 0, 0, :10]: tensor([-0.0958, -0.4826,  1.8254,  0.5603,  0.0250, -0.4494,  0.1387,  1.7155,\n",
            "         1.1604, -0.0771])\n",
            "\n",
            " DECODER_SELF ATTENTION\n",
            "Q shape: torch.Size([100, 1, 256]), K shape: torch.Size([100, 1, 256]), V shape: torch.Size([100, 1, 256])\n",
            "Per-head shape: Q torch.Size([100, 1, 8, 32]), K torch.Size([100, 1, 8, 32]), V torch.Size([100, 1, 8, 32])\n",
            " Q[0, 0, 0, :10]: tensor([ 0.2240, -0.0080, -0.0459,  0.1873,  0.1421, -0.2580, -0.1149,  0.0387,\n",
            "         0.2190,  0.0697])\n",
            " K[0, 0, 0, :10]: tensor([-0.2548,  2.0691, -0.6492,  0.3693, -0.3044,  2.5712, -3.0782, -0.2489,\n",
            "         1.0250,  0.4415])\n",
            " V[0, 0, 0, :10]: tensor([ 0.0940,  0.2145, -0.1791,  0.4920,  0.5498, -0.5215,  0.3167, -0.2200,\n",
            "        -0.0236, -0.5281])\n",
            "\n",
            " DECODER_CROSS ATTENTION\n",
            "Q shape: torch.Size([100, 1, 256]), K shape: torch.Size([1125, 1, 256]), V shape: torch.Size([1125, 1, 256])\n",
            "Per-head shape: Q torch.Size([100, 1, 8, 32]), K torch.Size([1125, 1, 8, 32]), V torch.Size([1125, 1, 8, 32])\n",
            " Q[0, 0, 0, :10]: tensor([-1.5874,  0.2910, -1.4245, -0.7688,  0.2009,  1.1497,  3.0290, -0.7988,\n",
            "        -0.2475, -0.9499])\n",
            " K[0, 0, 0, :10]: tensor([ 0.0626,  0.9934, -0.8089, -3.1269,  3.9636, -1.6424,  2.5630, -3.5774,\n",
            "         1.0490,  0.0069])\n",
            " V[0, 0, 0, :10]: tensor([ 0.5615, -0.5319, -0.0759,  0.0743, -0.0630, -0.0104, -0.7694,  0.3067,\n",
            "         0.2184,  0.9746])\n",
            "\n",
            " DECODER_SELF ATTENTION\n",
            "Q shape: torch.Size([100, 1, 256]), K shape: torch.Size([100, 1, 256]), V shape: torch.Size([100, 1, 256])\n",
            "Per-head shape: Q torch.Size([100, 1, 8, 32]), K torch.Size([100, 1, 8, 32]), V torch.Size([100, 1, 8, 32])\n",
            " Q[0, 0, 0, :10]: tensor([ 0.0758,  0.3196, -0.0665, -0.0479,  0.2717, -0.1816, -0.1199, -0.2736,\n",
            "         0.0051, -0.1142])\n",
            " K[0, 0, 0, :10]: tensor([-0.5533,  1.0329, -1.2378,  0.5016,  1.2312,  2.0189, -0.8813,  0.1639,\n",
            "         1.5935,  0.4857])\n",
            " V[0, 0, 0, :10]: tensor([ 0.3873, -0.2133, -0.1100, -0.4850, -0.2014,  0.0104,  0.0903, -0.3675,\n",
            "         0.0421,  0.3133])\n",
            "\n",
            " DECODER_CROSS ATTENTION\n",
            "Q shape: torch.Size([100, 1, 256]), K shape: torch.Size([1125, 1, 256]), V shape: torch.Size([1125, 1, 256])\n",
            "Per-head shape: Q torch.Size([100, 1, 8, 32]), K torch.Size([1125, 1, 8, 32]), V torch.Size([1125, 1, 8, 32])\n",
            " Q[0, 0, 0, :10]: tensor([-0.3822,  1.1715, -0.9414,  1.3091,  0.1654,  1.8701,  1.0061,  1.9156,\n",
            "        -0.4057, -0.2892])\n",
            " K[0, 0, 0, :10]: tensor([ 3.5440,  3.6295, -3.8800,  4.7839, -1.8860,  4.3622,  5.7188,  3.8415,\n",
            "        -2.8223, -0.4458])\n",
            " V[0, 0, 0, :10]: tensor([-0.0901,  0.2005, -0.6079,  0.0579,  0.0019,  0.0920,  0.1155, -0.9484,\n",
            "        -0.5470,  0.6254])\n",
            "\n",
            " DECODER_SELF ATTENTION\n",
            "Q shape: torch.Size([100, 1, 256]), K shape: torch.Size([100, 1, 256]), V shape: torch.Size([100, 1, 256])\n",
            "Per-head shape: Q torch.Size([100, 1, 8, 32]), K torch.Size([100, 1, 8, 32]), V torch.Size([100, 1, 8, 32])\n",
            " Q[0, 0, 0, :10]: tensor([-0.0850, -0.0236, -0.2171,  0.0883,  0.1486, -0.2301, -0.2908,  0.1798,\n",
            "         0.0972,  0.0833])\n",
            " K[0, 0, 0, :10]: tensor([ 0.2773,  1.6810,  0.7845, -0.4746,  0.9480,  0.8654, -2.0478,  1.1359,\n",
            "         0.7487,  0.7316])\n",
            " V[0, 0, 0, :10]: tensor([ 0.5426,  0.1453, -0.5724, -0.1397, -0.0872,  0.2405,  0.2277,  0.3947,\n",
            "        -0.0818, -0.3151])\n",
            "\n",
            " DECODER_CROSS ATTENTION\n",
            "Q shape: torch.Size([100, 1, 256]), K shape: torch.Size([1125, 1, 256]), V shape: torch.Size([1125, 1, 256])\n",
            "Per-head shape: Q torch.Size([100, 1, 8, 32]), K torch.Size([1125, 1, 8, 32]), V torch.Size([1125, 1, 8, 32])\n",
            " Q[0, 0, 0, :10]: tensor([ 1.9184, -1.7465,  1.2576,  2.7815, -0.6087,  1.5827,  1.5812, -0.0964,\n",
            "        -2.1091, -1.3962])\n",
            " K[0, 0, 0, :10]: tensor([ 2.7811,  1.2931, -4.9492, -1.0637, -5.3045,  1.0620,  1.4675, -1.6894,\n",
            "        -4.1499,  1.7942])\n",
            " V[0, 0, 0, :10]: tensor([ 0.4196, -0.1247,  0.4047, -0.4244, -0.1273,  0.0281,  0.2753, -0.3261,\n",
            "        -0.5360, -0.2214])\n",
            "\n",
            " DECODER_SELF ATTENTION\n",
            "Q shape: torch.Size([100, 1, 256]), K shape: torch.Size([100, 1, 256]), V shape: torch.Size([100, 1, 256])\n",
            "Per-head shape: Q torch.Size([100, 1, 8, 32]), K torch.Size([100, 1, 8, 32]), V torch.Size([100, 1, 8, 32])\n",
            " Q[0, 0, 0, :10]: tensor([ 0.4202, -0.5210, -0.0592, -0.5027,  0.3135,  0.2828, -0.0972,  0.4797,\n",
            "        -0.2565, -0.7809])\n",
            " K[0, 0, 0, :10]: tensor([ 0.8674, -7.4102, -0.4532, -4.7997,  4.2043,  0.4994, -3.9132,  2.8219,\n",
            "        -0.5348, -9.5293])\n",
            " V[0, 0, 0, :10]: tensor([-0.0807,  0.2956,  0.1535, -0.0757,  0.1805, -0.0317, -0.2187,  0.0933,\n",
            "        -0.0176, -0.3659])\n",
            "\n",
            " DECODER_CROSS ATTENTION\n",
            "Q shape: torch.Size([100, 1, 256]), K shape: torch.Size([1125, 1, 256]), V shape: torch.Size([1125, 1, 256])\n",
            "Per-head shape: Q torch.Size([100, 1, 8, 32]), K torch.Size([1125, 1, 8, 32]), V torch.Size([1125, 1, 8, 32])\n",
            " Q[0, 0, 0, :10]: tensor([ 1.3655,  1.3373, -1.1404, -1.2366,  0.1976,  0.5726, -0.2687, -1.0851,\n",
            "         0.6886, -0.8305])\n",
            " K[0, 0, 0, :10]: tensor([-3.1114,  0.6743, -1.5206, -0.9736,  2.2012, -2.3336,  1.3494, -0.0457,\n",
            "         0.1594,  1.5146])\n",
            " V[0, 0, 0, :10]: tensor([ 0.5534, -0.1978, -0.1122, -0.4766,  0.1778, -0.2533, -0.1669,  0.1286,\n",
            "        -0.2914,  0.1192])\n",
            "\n",
            " DECODER_SELF ATTENTION\n",
            "Q shape: torch.Size([100, 1, 256]), K shape: torch.Size([100, 1, 256]), V shape: torch.Size([100, 1, 256])\n",
            "Per-head shape: Q torch.Size([100, 1, 8, 32]), K torch.Size([100, 1, 8, 32]), V torch.Size([100, 1, 8, 32])\n",
            " Q[0, 0, 0, :10]: tensor([-0.2203,  0.0158, -0.0808, -0.2303, -0.1131,  0.1881, -0.0441, -0.1629,\n",
            "         0.0146, -0.2795])\n",
            " K[0, 0, 0, :10]: tensor([ 0.8270, -0.8905, -0.5980, -0.1306, -0.9067,  0.6085,  0.6665,  1.2631,\n",
            "        -0.8407,  0.5399])\n",
            " V[0, 0, 0, :10]: tensor([ 0.0855, -0.0957, -0.2690, -0.0323, -0.1047, -0.1431, -0.0387, -0.0466,\n",
            "         0.0187, -0.1371])\n",
            "\n",
            " DECODER_CROSS ATTENTION\n",
            "Q shape: torch.Size([100, 1, 256]), K shape: torch.Size([1125, 1, 256]), V shape: torch.Size([1125, 1, 256])\n",
            "Per-head shape: Q torch.Size([100, 1, 8, 32]), K torch.Size([1125, 1, 8, 32]), V torch.Size([1125, 1, 8, 32])\n",
            " Q[0, 0, 0, :10]: tensor([ 0.5703, -0.0077, -1.0565,  0.5561, -0.4535, -0.9865,  0.6018, -0.1267,\n",
            "        -0.0612,  0.5164])\n",
            " K[0, 0, 0, :10]: tensor([-2.0118, -0.3774, -0.9364, -0.3395, -3.6046,  3.5193, -3.0159,  0.0721,\n",
            "        -1.1110, -2.2231])\n",
            " V[0, 0, 0, :10]: tensor([-0.1143,  0.2378,  0.3642, -0.0392,  0.2329, -0.3178, -0.1116, -0.2273,\n",
            "        -0.1312, -0.0267])\n"
          ]
        }
      ]
    }
  ]
}